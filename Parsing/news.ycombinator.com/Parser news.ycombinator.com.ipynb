{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from time import sleep, time\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare some constants:\n",
    "\n",
    "filename = 'articles_info.csv' \n",
    "driver_path = '/Users/anna.zemit/chromedriver'\n",
    "base_dir= '/Users/anna.zemit/Desktop' \n",
    "user_agent = 'Mozilla/5.0' \n",
    "start_time = time() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how long it takes to load each web page\n",
    "\n",
    "def get_load_time(article_url, user_agent):\n",
    "    \n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": user_agent\n",
    "        }\n",
    "        response = requests.get(\n",
    "            article_url, headers=headers, stream=True, timeout=3.000\n",
    "        )\n",
    "        load_time = response.elapsed.total_seconds()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        load_time = \">3\"\n",
    "    return load_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a file\n",
    "\n",
    "def write_to_file(output_list, filename, base_dir):\n",
    "    for row in output_list:\n",
    "        with open(Path(base_dir).joinpath(filename), \"a\") as csvfile:\n",
    "            fieldnames = ['id', 'load_time', 'rank', 'points', 'title', 'url', 'comments_number']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method waits for the table to be loaded onto the page and returns TRUE if it is available.\n",
    "\n",
    "def connect_to_base(browser, page_number):\n",
    "    base_url = \"https://news.ycombinator.com/news?p={}\".format(page_number)\n",
    "    for connection_attempts in range(1,4): \n",
    "        try:\n",
    "            browser.get(base_url)\n",
    "            WebDriverWait(browser, 5).until(\n",
    "                EC.presence_of_element_located((By.ID, \"hnmain\"))\n",
    "            )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Error connecting to {}.\".format(base_url))\n",
    "            print(\"Attempt #{}.\".format(connection_attempts))\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the page, extracting the necessary attributes and saving them\n",
    "\n",
    "def parse_html(html, user_agent):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    output_list = []\n",
    "    \n",
    "# find object id, rank, score and title in soup\n",
    "    tr_blocks = soup.find_all(\"tr\", class_=\"athing\")\n",
    "    subtext = soup.find_all(\"td\", class_=\"subtext\")\n",
    "    article = 0\n",
    "    for tr in tr_blocks:\n",
    "        article_id = tr.get(\"id\") # id\n",
    "        article_url = tr.find_all(\"a\")[1][\"href\"]\n",
    "\n",
    "# sometimes the article is not located on an external site\n",
    "        if \"item?id=\" in article_url or \"from?site=\" in article_url:\n",
    "            article_url = f\"https://news.ycombinator.com/{article_url}\"\n",
    "        load_time = get_load_time(article_url, user_agent)\n",
    "        \n",
    "# sometimes there is no rating\n",
    "        try:\n",
    "            score = soup.find(id=f\"score_{article_id}\").string\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            score = \"0 points\"\n",
    "            \n",
    "# find a comments number\n",
    "    \n",
    "        try:\n",
    "            comments_number = int(re.findall('([0-9]+)\\xa0comment[s]*', subtext[article].text)[0])\n",
    "        except Exception as e:\n",
    "            comments_number = 0\n",
    "\n",
    "        article_info = {\n",
    "            \"id\": article_id,\n",
    "            \"load_time\": load_time,\n",
    "            \"rank\": tr.span.string,\n",
    "            \"points\": score,\n",
    "            \"title\": tr.find(class_=\"titlelink\").string,\n",
    "            \"url\": article_url,\n",
    "            'comments_number': comments_number\n",
    "        }\n",
    "\n",
    "# add information about the article to the list\n",
    "        output_list.append(article_info)\n",
    "        article += 1\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_process(page_number, filename):\n",
    "    browser = webdriver.Chrome(executable_path=driver_path)\n",
    "    if connect_to_base(browser, page_number):\n",
    "        sleep(5)\n",
    "        output_list = parse_html(browser.page_source, user_agent)\n",
    "        write_to_file(output_list, filename, base_dir)\n",
    "        \n",
    "        browser.quit()\n",
    "    else:\n",
    "        print(\"Error connecting to hacker news\")\n",
    "        browser.quit()\n",
    "\n",
    "futures = []\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for number in range(2):\n",
    "        futures.append(\n",
    "            executor.submit(run_process, number, filename)\n",
    "        )\n",
    "wait(futures)\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed run time: {} seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
